{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDI343\n",
    "## Lab on real-world graph analysis -- graph neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this lab is to get a feeling of real-world graphs. For information on the `scikit-network` library, [the documentation is handy](https://scikit-network.readthedocs.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sknetwork as skn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util function to plot the inverse cumulative distribution\n",
    "def ccdf(values):\n",
    "    x = []\n",
    "    y = []\n",
    "    values = sorted(values)\n",
    "\n",
    "    # First make dist\n",
    "    dist = [(key, len(list(group))) for key, group in groupby(values)]\n",
    "\n",
    "    # Then compute inverse cumulative\n",
    "    total = 1.0\n",
    "    for (val, count) in dist:\n",
    "        x.append(val)\n",
    "        y.append(total)\n",
    "        total -= count/len(values)\n",
    "    return x, y\n",
    "\n",
    "# Util function to return the distribution of values\n",
    "def dist(values):\n",
    "    values = sorted(values)\n",
    "\n",
    "    # First make dist\n",
    "    dist = [(key, len(list(group))) for key, group in groupby(values)]\n",
    "    \n",
    "    return [x[0] for x in dist], [x[1] for x in dist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work on 2 graphs induced by the [Vital articles of Wikipedia](https://en.wikipedia.org/wiki/Wikipedia:Vital_articles/Level/4), a selection of about 10,000 articles of the English Wikipedia:\n",
    "* the directed graph of hyperlinks between these articles,\n",
    "* the bipartite graph between articles and (stemmed) words used in their summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['adjacency', 'biadjacency', 'names', 'names_row', 'names_col', 'labels', 'labels_hierarchy', 'names_labels', 'names_labels_hierarchy', 'meta'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = skn.data.load_netset('wikivitals')\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# graph of links\n",
    "adjacency = dgl.from_scipy(data.adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph of words\n",
    "biadjacency = dgl.bipartite_from_scipy(data.biadjacency, \"articles\", \"words\", \"occurrence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article names\n",
    "names = data.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['People', 'History', 'Geography', 'Arts',\n",
       "       'Philosophy and religion', 'Everyday life',\n",
       "       'Society and social sciences', 'Biology and health sciences',\n",
       "       'Physical sciences', 'Technology', 'Mathematics'], dtype='<U27')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# article categories\n",
    "categories = data.names_labels\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['moos', 'tonnag', 'separatist', ..., 'luteum', 'radiat', 'helena'],\n",
       "      dtype='<U22')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words\n",
    "words = data.names_col\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_index = {name:i for i, name in enumerate(names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words, num_articles = biadjacency.num_dst_nodes(), biadjacency.num_src_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "For the 2 graphs:\n",
    "* Separate the data into training and validation sets\n",
    "* Fill the code to implement a GCN with the deep graph library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_msg = fn.copy_src(src='h', out='m')\n",
    "gcn_reduce = fn.sum(msg='m', out='h')\n",
    "\n",
    "# Change for Wikipedia\n",
    "num_features = 1433\n",
    "num_classes = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, features_in, features_out):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(features_in, features_out)\n",
    "        \n",
    "    def forward(self, g, feature):\n",
    "        with g.local_scope():\n",
    "            g.ndata[\"h\"] = feature\n",
    "            g.update_all(gcn_msg, gcn_reduce)\n",
    "            h = g.ndata[\"h\"]\n",
    "            return self.linear(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCNet, self).__init__()\n",
    "        self.layer1 = GCNLayer(num_features, 16)\n",
    "        self.layer2 = GCNLayer(16, num_classes)\n",
    "        \n",
    "    def forward(self, g, features):\n",
    "        x = F.relu(self.layer1(g, features))\n",
    "        x = self.layer2(g, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCNet(\n",
      "  (layer1): GCNLayer(\n",
      "    (linear): Linear(in_features=1433, out_features=16, bias=True)\n",
      "  )\n",
      "  (layer2): GCNLayer(\n",
      "    (linear): Linear(in_features=16, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = GCNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use some more common dataset, just to get a hang of how things work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import citation_graph as citegrh\n",
    "def load_cora_data():\n",
    "    data = citegrh.load_cora()\n",
    "    features = th.FloatTensor(data.features)\n",
    "    labels = th.LongTensor(data.labels)\n",
    "    train_mask = th.BoolTensor(data.train_mask)\n",
    "    test_mask = th.BoolTensor(data.test_mask)\n",
    "    g = dgl.from_networkx(data.graph)\n",
    "    return g, features, labels, train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, g, features, labels, mask):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = th.max(logits, dim=1)\n",
    "        correct = th.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache failed, re-processing.\n",
      "Finished data loading and preprocessing.\n",
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done saving data into cached files.\n",
      "Epoch 00000 | Loss 0.0104 | Accuracy on test 0.7500\n",
      "Epoch 00001 | Loss 0.0077 | Accuracy on test 0.7370\n",
      "Epoch 00002 | Loss 0.0060 | Accuracy on test 0.7350\n",
      "Epoch 00003 | Loss 0.0046 | Accuracy on test 0.7420\n",
      "Epoch 00004 | Loss 0.0034 | Accuracy on test 0.7430\n",
      "Epoch 00005 | Loss 0.0026 | Accuracy on test 0.7390\n",
      "Epoch 00006 | Loss 0.0020 | Accuracy on test 0.7370\n",
      "Epoch 00007 | Loss 0.0016 | Accuracy on test 0.7350\n",
      "Epoch 00008 | Loss 0.0012 | Accuracy on test 0.7350\n",
      "Epoch 00009 | Loss 0.0010 | Accuracy on test 0.7320\n",
      "Epoch 00010 | Loss 0.0008 | Accuracy on test 0.7330\n",
      "Epoch 00011 | Loss 0.0006 | Accuracy on test 0.7330\n",
      "Epoch 00012 | Loss 0.0005 | Accuracy on test 0.7300\n",
      "Epoch 00013 | Loss 0.0004 | Accuracy on test 0.7300\n",
      "Epoch 00014 | Loss 0.0004 | Accuracy on test 0.7310\n",
      "Epoch 00015 | Loss 0.0003 | Accuracy on test 0.7300\n",
      "Epoch 00016 | Loss 0.0003 | Accuracy on test 0.7310\n",
      "Epoch 00017 | Loss 0.0002 | Accuracy on test 0.7320\n",
      "Epoch 00018 | Loss 0.0002 | Accuracy on test 0.7320\n",
      "Epoch 00019 | Loss 0.0002 | Accuracy on test 0.7320\n",
      "Epoch 00020 | Loss 0.0001 | Accuracy on test 0.7330\n",
      "Epoch 00021 | Loss 0.0001 | Accuracy on test 0.7340\n",
      "Epoch 00022 | Loss 0.0001 | Accuracy on test 0.7350\n",
      "Epoch 00023 | Loss 0.0001 | Accuracy on test 0.7360\n",
      "Epoch 00024 | Loss 0.0001 | Accuracy on test 0.7370\n",
      "Epoch 00025 | Loss 0.0001 | Accuracy on test 0.7370\n",
      "Epoch 00026 | Loss 0.0001 | Accuracy on test 0.7370\n",
      "Epoch 00027 | Loss 0.0001 | Accuracy on test 0.7360\n",
      "Epoch 00028 | Loss 0.0001 | Accuracy on test 0.7370\n",
      "Epoch 00029 | Loss 0.0001 | Accuracy on test 0.7380\n",
      "Epoch 00030 | Loss 0.0001 | Accuracy on test 0.7390\n",
      "Epoch 00031 | Loss 0.0001 | Accuracy on test 0.7400\n",
      "Epoch 00032 | Loss 0.0001 | Accuracy on test 0.7390\n",
      "Epoch 00033 | Loss 0.0001 | Accuracy on test 0.7410\n",
      "Epoch 00034 | Loss 0.0001 | Accuracy on test 0.7410\n",
      "Epoch 00035 | Loss 0.0000 | Accuracy on test 0.7410\n",
      "Epoch 00036 | Loss 0.0000 | Accuracy on test 0.7390\n",
      "Epoch 00037 | Loss 0.0000 | Accuracy on test 0.7390\n",
      "Epoch 00038 | Loss 0.0000 | Accuracy on test 0.7390\n",
      "Epoch 00039 | Loss 0.0000 | Accuracy on test 0.7390\n",
      "Epoch 00040 | Loss 0.0000 | Accuracy on test 0.7390\n",
      "Epoch 00041 | Loss 0.0000 | Accuracy on test 0.7380\n",
      "Epoch 00042 | Loss 0.0000 | Accuracy on test 0.7380\n",
      "Epoch 00043 | Loss 0.0000 | Accuracy on test 0.7380\n",
      "Epoch 00044 | Loss 0.0000 | Accuracy on test 0.7380\n",
      "Epoch 00045 | Loss 0.0000 | Accuracy on test 0.7380\n",
      "Epoch 00046 | Loss 0.0000 | Accuracy on test 0.7370\n",
      "Epoch 00047 | Loss 0.0000 | Accuracy on test 0.7370\n",
      "Epoch 00048 | Loss 0.0000 | Accuracy on test 0.7370\n",
      "Epoch 00049 | Loss 0.0000 | Accuracy on test 0.7370\n"
     ]
    }
   ],
   "source": [
    "g, features, labels, train_mask, test_mask = load_cora_data()\n",
    "\n",
    "# Add edges between each node and itself to preserve old node representations\n",
    "g.add_edges(g.nodes(), g.nodes())\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-2)\n",
    "\n",
    "for epoch in range(50):\n",
    "\n",
    "    net.train()\n",
    "    logits = net(g, features)\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[train_mask], labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    acc = evaluate(net, g, features, labels, test_mask)\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Accuracy on test {:.4f}\".format(\n",
    "            epoch, loss.item(), acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo\n",
    "\n",
    " * Separate our dataset into train and test (you just need to define a `train_mask` and a `test_mask`, which are Boolean vectors)\n",
    " * Adapt the wikipedia dataset to run our GCN model\n",
    " * Use the GCN to \"find out\" the article category of the articles in the test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for features, you can use a vector of 0/1 indicating the absence/presence of a word in a given article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wikivitals_data():\n",
    "    % \n",
    "    return g, features, labels, train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
