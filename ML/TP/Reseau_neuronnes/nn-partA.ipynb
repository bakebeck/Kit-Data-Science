{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS Big Data\n",
    "## TP on Neural Networks (part A) 19/01/2021\n",
    "## Coding Multi-Layer Perceptron in python (for a binary classification problem)\n",
    "\n",
    "For any remark or suggestion, please feel free to contact:\n",
    "geoffroy.peeters@telecom-paristech.fr\n",
    "\n",
    "### Objective:\n",
    "We want to implement a two layers MLP (1 hidden layer) in Python.\n",
    "\n",
    "#### Forward propagation\n",
    "\n",
    "- $Z^{[1]} = W^{[1]} X + b^{[1]}$\n",
    "- $A^{[1]} = g^{[1]}(Z^{[1]})$\n",
    "- $Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$\n",
    "- $A^{[2]} = g^{[2]}(Z^{[2]})$\n",
    "\n",
    "where \n",
    "- $g^{[1]}$ is a ```Relu``` function (the code is provided)\n",
    "- $g^{[2]}$ is a sigmoid function (the code is provided)\n",
    "\n",
    "#### Compute the cost \n",
    "\n",
    "The cost is average of the the loss over the training data. Since we deal with a binary classification problem, we will use the binary cross-entropy.\n",
    "\n",
    "$\\mathcal{L} = - \\left( y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y}) \\right)$\n",
    "\n",
    "#### Backward propagation\n",
    "\n",
    "Based on the slides of the lecture, write the corresponding backward propagation algorithm.\n",
    "\n",
    "#### Parameters update\n",
    "\n",
    "- Implement a **first version** in which the parameters are updated using a simple gradient descent:\n",
    "- $W = W + \\alpha dW$\n",
    "\n",
    "\n",
    "- Implement a **second version** in which the parameters are updated using the momentum method\n",
    "- $VdW(t) = \\beta VdW(t-1) + (1-\\beta) dW$\n",
    "- $W) = W + \\alpha VdW(t)$\n",
    "\n",
    "We therefore need to implement the\n",
    "\n",
    "\n",
    "### Your task:\n",
    "\n",
    "You need to add the missing parts in the code (parts between ```# --- START CODE HERE``` and ```# --- END CODE HERE```)\n",
    "\n",
    "### Note \n",
    "\n",
    "The code is written as a python class (in order to be able to pass all the variables easely from one function to the other).\n",
    "\n",
    "To use a given variable, you need to use ```self.$VARIABLE_NAME```, such as````self.W1````,```self.b1```, ... (see the code already written).\n",
    "\n",
    "### Testing\n",
    "\n",
    "For testing your code, you can use the code provided in the last cells (loop over epochs and display of the loss decrease).\n",
    "You should a cost which decreases (largely) over epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define a set of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_standardize(X):\n",
    "    \"\"\"\n",
    "    standardize X, i.e. subtract mean (over data) and divide by standard-deviation (over data)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.array of size (nbData, nbDim)\n",
    "        matrix containing the observation data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X: np.array of size (nbData, nbDim)\n",
    "        standardize version of X\n",
    "    \"\"\"\n",
    "    \n",
    "    X -= np.mean(X, axis=0, keepdims=True) \n",
    "    X /= (np.std(X, axis=0, keepdims=True) + 1e-16)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_sigmoid(x):\n",
    "    \"\"\"Compute the value of the sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def F_relu(x):\n",
    "    \"\"\"Compute the value of the Rectified Linear Unit activation function\"\"\"\n",
    "    return x * (x > 0)\n",
    "\n",
    "def F_dRelu(x):\n",
    "    \"\"\"Compute the derivative of the Rectified Linear Unit activation function\"\"\"\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def F_computeCost(hat_y, y):\n",
    "    \"\"\"Compute the cost (sum of the losses)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hat_y: (1, nbData)\n",
    "        predicted value by the MLP\n",
    "    y: (1, nbData)\n",
    "        ground-truth class to predict\n",
    "    \"\"\"\n",
    "    m = y.shape[1]\n",
    "     \n",
    "    # --- START CODE HERE\n",
    "    loss = ...\n",
    "    # --- END CODE HERE\n",
    "    \n",
    "    cost = np.sum(loss) / m\n",
    "    return cost\n",
    "\n",
    "def F_computeAccuracy(hat_y, y):\n",
    "    \"\"\"Compute the accuracy\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hat_y: (1, nbData)\n",
    "        predicted value by the MLP\n",
    "    y: (1, nbData)\n",
    "        ground-truth class to predict\n",
    "    \"\"\"\n",
    "    \n",
    "    m = y.shape[1]    \n",
    "    class_y = np.copy(hat_y)\n",
    "    class_y[class_y>=0.5]=1\n",
    "    class_y[class_y<0.5]=0\n",
    "    return np.sum(class_y==y) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load dataset and pre-process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_circles(n_samples=1000, noise=0.05, factor=0.5)\n",
    "\n",
    "print(\"X.shape: {}\".format(X.shape))\n",
    "print(\"y.shape: {}\".format(y.shape))\n",
    "print(set(y))\n",
    "\n",
    "# X is (nbExamples, nbDim)\n",
    "# y is (nbExamples,)\n",
    "\n",
    "# --- Standardize data\n",
    "X = F_standardize(X)\n",
    "\n",
    "# --- Split between training set and test set\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# --- Convert to proper shape: (nbExamples, nbDim) -> (nbDim, nbExamples)\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "\n",
    "# --- Convert to proper shape: (nbExamples,) -> (1, nbExamples)\n",
    "y_train = y_train.reshape(1, len(y_train))\n",
    "y_test = y_test.reshape(1, len(y_test))\n",
    "\n",
    "# --- Convert to oneHotEncoding: (1, nbExamples) -> (nbClass, nbExamples)\n",
    "n_in = X_train.shape[0]\n",
    "n_out = 1\n",
    "\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "print(\"X_test.shape: {}\".format(X_test.shape))\n",
    "print(\"y_train.shape: {}\".format(y_train.shape))\n",
    "print(\"y_test.shape: {}\".format(y_test.shape))\n",
    "print(\"y_train.shape: {}\".format(y_train.shape))\n",
    "print(\"y_test.shape: {}\".format(y_test.shape))\n",
    "print(\"n_in: {} n_out: {}\".format(n_in, n_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define the MLP class with forward, backward and update methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C_MultiLayerPerceptron:\n",
    "    \"\"\"\n",
    "    A class used to represent a Multi-Layer Perceptron with 1 hidden layers\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    W1, b1, W2, b2:\n",
    "        weights and biases to be learnt\n",
    "    Z1, A1, Z2, A2:\n",
    "        values of the internal neurons to be used for backpropagation\n",
    "    dW1, db1, dW2, db2, dZ1, dZ2:\n",
    "        partial derivatives of the loss w.r.t. parameters\n",
    "    VdW1, Vdb1, VdW2, Vdb2:\n",
    "        momentum terms\n",
    "    do_bin0_multi1:\n",
    "        set wether we solve a binary or a multi-class classification problem\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    forward_propagation\n",
    "    \n",
    "    backward_propagation\n",
    "    \n",
    "    update_parameters\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    W1, b1, W2, b2 = [], [], [], []\n",
    "    Z1, A1, Z2, A2 = [], [], [], []\n",
    "    dW1, db1, dW2, db2 = [], [], [], []   \n",
    "    dZ1, dA1, dZ2 = [], [], []\n",
    "    # --- for momentum\n",
    "    VdW1, Vdb1, VdW2, Vdb2 = [], [], [], []     \n",
    "    \n",
    "    def __init__(self, n_in, n_h, n_out):\n",
    "        self.W1 = np.random.randn(n_h, n_in) * 0.01\n",
    "        self.b1 = np.zeros(shape=(n_h, 1))\n",
    "        self.W2 = np.random.randn(n_out, n_h) * 0.01\n",
    "        self.b2 = np.zeros(shape=(n_out, 1))        \n",
    "        # --- for momentum\n",
    "        self.VdW1 = np.zeros(shape=(n_h, n_in)) \n",
    "        self.Vdb1 = np.zeros(shape=(n_h, 1))\n",
    "        self.VdW2 = np.zeros(shape=(n_out, n_h))\n",
    "        self.Vdb2 = np.zeros(shape=(n_out, 1))\n",
    "        return\n",
    "\n",
    "    \n",
    "    def __setattr__(self, attrName, val):\n",
    "        if hasattr(self, attrName):\n",
    "            self.__dict__[attrName] = val\n",
    "        else:\n",
    "            raise Exception(\"self.%s note part of the fields\" % attrName)\n",
    "\n",
    "            \n",
    "\n",
    "    def M_forwardPropagation(self, X):\n",
    "        \"\"\"Forward propagation in the MLP\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy array (nbDim, nbData)\n",
    "            observation data\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        hat_y: numpy array (1, nbData)\n",
    "            predicted value by the MLP\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- START CODE HERE\n",
    "        self.Z1 = ...\n",
    "        self.A1 = ...\n",
    "        self.Z2 = ...\n",
    "        self.A2 = ...\n",
    "        # --- END CODE HERE\n",
    "        \n",
    "        hat_y = self.A2\n",
    "        \n",
    "        return hat_y\n",
    "\n",
    "\n",
    "    def M_backwardPropagation(self, X, y):\n",
    "        \"\"\"Backward propagation in the MLP\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy array (nbDim, nbData)\n",
    "            observation data\n",
    "        y: numpy array (1, nbData)\n",
    "            ground-truth class to predict\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        m = y.shape[1]\n",
    "        \n",
    "        # --- START CODE HERE\n",
    "        self.dZ2 = ...\n",
    "        self.dW2 = ...\n",
    "        self.db2 = ...\n",
    "        self.dA1 = ...\n",
    "        self.dZ1 = ...\n",
    "        self.dW1 = ...\n",
    "        self.db1 = ...\n",
    "        # --- END CODE HERE\n",
    "        return\n",
    "\n",
    "    \n",
    "    def M_gradientDescent(self, alpha):\n",
    "        \"\"\"Update the parameters of the network using gradient descent\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha: float scalar\n",
    "            amount of update at each step of the gradient descent\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        # --- START CODE HERE\n",
    "        self.W1 = ...\n",
    "        self.b1 = ...\n",
    "        self.W2 = ...\n",
    "        self.b2 = ...\n",
    "        # --- END CODE HERE\n",
    "        \n",
    "        return\n",
    "\n",
    "    \n",
    "    def M_momentum(self, alpha, beta):\n",
    "        \"\"\"Update the parameters of the network using momentum method\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha: float scalar\n",
    "            amount of update at each step of the gradient descent\n",
    "        beta: float scalar\n",
    "            momentum term \n",
    "        \"\"\"\n",
    "        \n",
    "        # --- START CODE HERE\n",
    "        self.VdW1 = ...\n",
    "        self.W1 = ...\n",
    "        self.Vdb1 = ...\n",
    "        self.b1 = ...\n",
    "        self.VdW2 = ...\n",
    "        self.W2 = ...\n",
    "        self.Vdb2 = ...\n",
    "        self.b2 = ...\n",
    "        # --- END CODE HERE\n",
    "                \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Perform training using batch-gradiant and epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the class MLP with providing \n",
    "# the size of the various layers (input=4, hidden=10, outout=1) \n",
    "\n",
    "n_hidden = 10\n",
    "num_epoch = 5000\n",
    "\n",
    "\n",
    "myMLP = C_MultiLayerPerceptron(n_in, n_hidden, n_out)\n",
    "\n",
    "train_cost, train_accuracy, test_cost, test_accuracy = [], [], [], []\n",
    "\n",
    "# Run over epochs\n",
    "for i in range(0, num_epoch):\n",
    "    \n",
    "    # --- Forward\n",
    "    hat_y_train = myMLP.M_forwardPropagation(X_train)\n",
    "    \n",
    "    # --- Store results on train\n",
    "    train_cost.append( F_computeCost(hat_y_train, y_train) )\n",
    "    train_accuracy.append( F_computeAccuracy(hat_y_train, y_train) )\n",
    "    \n",
    "    # --- Backward\n",
    "    myMLP.M_backwardPropagation(X_train, y_train)\n",
    "    \n",
    "    # --- Update\n",
    "    myMLP.M_gradientDescent(alpha=0.1)\n",
    "    #myMLP.M_momentum(alpha=0.1, beta=0.9)\n",
    "\n",
    "    # --- Store results on test\n",
    "    hat_y_test = myMLP.M_forwardPropagation(X_test)\n",
    "    test_cost.append( F_computeCost(hat_y_test, y_test) )    \n",
    "    test_accuracy.append( F_computeAccuracy(hat_y_test, y_test) )\n",
    "    \n",
    "    if (i % 500)==0: \n",
    "        print(\"epoch: {0:d} (cost: train {1:.2f} test {2:.2f}) (accuracy: train {3:.2f} test {4:.2f})\".format(i, train_cost[-1], test_cost[-1], train_accuracy[-1], test_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display train/test loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_cost, 'r')\n",
    "plt.plot(test_cost, 'g--')\n",
    "plt.xlabel('# epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.grid(True)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_accuracy, 'r')\n",
    "plt.plot(test_accuracy, 'g--')\n",
    "plt.xlabel('# epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
