{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute LDA classifier\n",
    "# coded in Python\n",
    "import numpy as np\n",
    "# allows us to multiply more than one vector at once\n",
    "from numpy.linalg import multi_dot\n",
    "# allows us to compute inverse of matrix\n",
    "from numpy.linalg import inv\n",
    "\n",
    "\n",
    "def LDA_classifier(X,estimates,variance_estimate):\n",
    "\t\"\"\"\n",
    "\tA function to return LDA classification output for a given X\n",
    "\tWe use a vectorized implementation so that we can avoid\n",
    "\tfor loops and speed up computation time\n",
    "\t@ X: input training data\n",
    "\t@ estimates: list of tuples that contain parameter estimates\n",
    "\ttuples are in the form (class,pi,mean,variance) BUT WE DISREGARD the variance\n",
    "\t@ variance_estimate: the variance estimate we use for the classifier\n",
    "\t\"\"\"\n",
    "\n",
    "\t# list of column vectors that contain bayes (log) probabilities for each class\n",
    "\t# we will eventually concatenate the output and predict the class that\n",
    "\t# has the highest probability\n",
    "\n",
    "\tbayes_probabilities = []\n",
    "\n",
    "\t# iterate through all estimates (which represents estimate for each class)\n",
    "\t# recall that each estimate is in in the form (class,pi,mean,variance)\n",
    "\tfor estimate in estimates:\n",
    "\n",
    "\t\tpi = estimate[1]\n",
    "\t\tmean = estimate[2]\n",
    "\t\t# variance inverse \n",
    "\t\tsigma_inv = inv(variance_estimate)\n",
    "\n",
    "\t\t# formula for linear discriminant\n",
    "\t\t# the second and third terms are BROADCASTED across the first term, which is a vector\n",
    "\t\t# with shape (# of observations, # of features)\n",
    "\t\tbayes_prob = multi_dot([X,sigma_inv,mean]) - (.5 * multi_dot([mean.T,sigma_inv,mean])) + np.log(pi)\n",
    "\n",
    "\t\t# appends a \n",
    "\t\tbayes_probabilities.append(bayes_prob)\n",
    "\n",
    "\t# now we will concatenate the probabilities for each class\n",
    "\t# and take the argmax, to find the index that had the highest\n",
    "\t# log probability.\n",
    "\n",
    "\t# for example, if the 3rd log probability (at index 2) of the first row \n",
    "\t# was the highest, then the first entry of this array will contain a '2'\n",
    "\n",
    "\tindices_of_highest_prob = np.argmax(np.concatenate(bayes_probabilities,axis=1),axis=1)\n",
    "\n",
    "\t# now predict the class based on the index of the highest log probability.\n",
    "\t# for example, if the index was '1', this means that the log probability was\n",
    "\t# highest for the second set of estimates, and so we predict the class assigned\n",
    "\t# to that estimate (this is why we included the class in the tuple!)\n",
    "\n",
    "\tdef predict_class(index):\n",
    "\t\t# the class is in the 0th index of the tuple\n",
    "\t\treturn estimates[index][0]\n",
    "\n",
    "\t# create a function that does this to a vector\n",
    "\tpredict_class_vec = np.vectorize(predict_class)\n",
    "\n",
    "\n",
    "\tpredictions = predict_class_vec(indices_of_highest_prob)\n",
    "\n",
    "\treturn predictions\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
